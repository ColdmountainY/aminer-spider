##短期报告-李翔

###任务：

Aminer从google scholar上取数据的爬虫很容易被封杀。如何修改爬虫使得它能开始工作。
分析：
1.先写了个最基本的爬虫，发送的是不带报头的请求，在某些网站上能成功被服务，但是google直接给干掉。
2.给爬虫加了报头，只添加了user-agent，可以成功访问google scholar了。两次测试（一次用实验室的ip一次用住处的ip）都是在四十次左右会被google检测到。会出现输入验证码才能继续访问的页面。
3.再给爬虫的报头中添加了cookie（从浏览器里拷贝出来的……），同样能成功获得数据，单线程一直跑下去，大概能搜索400次左右，然后出现拒绝服务。再弄到个新的cookie，然后发现可以用当前的ip继续跑了！

###推测：
Google很可能是限制每个ip每天最多访问多少次，但是次数应该很多，一个学校可能只有几个外网ip，通过这个最终ip访问次数肯定是不小，如果google封杀掉的话，错误封杀的可能会很大。
所以很可能是通过cookie来判断用户行为。开始报头里没有加cookie的时候，连续访问的比较频繁，所以google返回给输入验证码的页面，这应该是说明了他们在怀疑这个不是用户行为，但是用浏览器仍然是可以继续访问，说明很可能是cookie的问题。加上cookie之后能访问次数更多了，但是还是被封掉了，换个cookie又可以访问了！那一定是cookie的事情了。

搜索了浏览器的行为：第一次向服务器发送的时候是没有cookie的，所以服务器会返回一个键值对，set-cookie，这个值浏览器会取出来，之后再次访问该服务器的时候就会放进报头，让服务器能知道该用户的之前行为。
而原先的程序中只是用了一个固定的cookie，被封掉应该是情理之中……

###解决方法:
让爬虫模拟浏览器，在第一次要访问google的时候得到的cookie给存下来，之后利用这个cookie搜一定的次数。方法：利用库cookielib.cookjar
1：每次爬网页的时候开一个cookiejar。（初步跑起来没有被封杀，后面结果还待定）
2：如果方法1还是容易被封。在线程里了建立一个cookiejar，在一定次数后重新新建cookiejar，使得能够保存新的cookie（等第一个结果出再定）

###感想：
纸上学来的还在纸上，遇到困难之后学到的才在脑子里
纠结了两天找不到封杀原因，写了不少test，结果就两行真正解决问题。。。。。。略坑略坑

